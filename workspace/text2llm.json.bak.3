{
  "gpu": {
    "providers": {},
    "providerAccounts": [],
    "instances": [],
    "routing": {},
    "inferenceProfiles": {},
    "inferenceRequestLogs": [],
    "budgetPolicies": {},
    "reliability": {},
    "circuitBreakers": {},
    "fallbackRoutes": {},
    "observability": {},
    "rollout": {},
    "auditLogs": [],
    "kms": {}
  },
  "notebook": {
    "cells": [
      {
        "id": "cell-1772266723422-efnvg9",
        "projectId": "default",
        "type": "markdown",
        "source": "# Project Notebook\nCode cells generated by text2llm.",
        "outputs": [],
        "executionCount": null,
        "status": "idle",
        "createdAt": "2026-02-28T08:18:43.422Z",
        "updatedAt": "2026-02-28T08:18:43.422Z"
      },
      {
        "id": "cell-1772266723422-9rsv0q",
        "projectId": "default",
        "type": "code",
        "source": "# Ready for GPU inference\nprint('Hello from text2llm')",
        "outputs": [
          {
            "type": "stdout",
            "text": "Hello from text2llm\n"
          }
        ],
        "executionCount": 5,
        "status": "completed",
        "createdAt": "2026-02-28T08:18:43.422Z",
        "updatedAt": "2026-02-28T08:18:49.538Z"
      },
      {
        "id": "cell-1772266723422-b4c52h",
        "projectId": "default",
        "type": "code",
        "source": "!pip install transformers torch datasets -q",
        "outputs": [
          {
            "type": "stdout",
            "text": ""
          }
        ],
        "executionCount": 6,
        "status": "completed",
        "createdAt": "2026-02-28T08:18:43.422Z",
        "updatedAt": "2026-02-28T08:18:49.538Z"
      },
      {
        "id": "cell-1772266723433-73ya48",
        "projectId": "default",
        "type": "code",
        "source": "import torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntext = \"Artificial intelligence is transforming the way we work and live.\"\nencodings = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**encodings, labels=encodings[\"input_ids\"])\n    perplexity = torch.exp(outputs.loss).item()\nprint(f\"Text: {text}\")\nprint(f\"Perplexity: {perplexity:.2f}\")\nprint(\"Lower perplexity = better prediction\")",
        "outputs": [
          {
            "type": "stdout",
            "text": "Text: {text}\n"
          },
          {
            "type": "stdout",
            "text": "Perplexity: {perplexity:.2f}\n"
          },
          {
            "type": "stdout",
            "text": "Lower perplexity = better prediction\n"
          }
        ],
        "executionCount": 7,
        "status": "completed",
        "createdAt": "2026-02-28T08:18:43.433Z",
        "updatedAt": "2026-02-28T08:18:49.538Z"
      },
      {
        "id": "cell-1772266723430-0gz53e",
        "projectId": "default",
        "type": "code",
        "source": "import torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntext = \"Artificial intelligence is transforming the way we work and live.\"\nencodings = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**encodings, labels=encodings[\"input_ids\"])\n    perplexity = torch.exp(outputs.loss).item()\nprint(f\"Text: {text}\")\nprint(f\"Perplexity: {perplexity:.2f}\")\nprint(\"Lower perplexity = better prediction\")",
        "outputs": [
          {
            "type": "stdout",
            "text": "Text: {text}\n"
          },
          {
            "type": "stdout",
            "text": "Perplexity: {perplexity:.2f}\n"
          },
          {
            "type": "stdout",
            "text": "Lower perplexity = better prediction\n"
          }
        ],
        "executionCount": 8,
        "status": "completed",
        "createdAt": "2026-02-28T08:18:43.430Z",
        "updatedAt": "2026-02-28T08:18:49.538Z"
      }
    ]
  },
  "plugins": {
    "entries": {
      "google-gemini-cli-auth": {
        "enabled": true
      }
    }
  },
  "web": {
    "providers": {
      "selectedOptionByProvider": {
        "google": "cli"
      }
    }
  }
}
